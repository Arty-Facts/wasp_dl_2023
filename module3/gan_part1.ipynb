{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d35e9e"
      },
      "source": [
        "# GANs &ndash; Part 1: Generation\n",
        "\n",
        "Author: Sebastian Bujwid <bujwid@kth.se>\n",
        "\n",
        "Refer to [./README.md](./README.md) for general comments.\n",
        "\n",
        "---\n",
        "\n",
        "## Scope\n",
        "\n",
        "- Implementing a GAN model\n",
        "- Training a GAN on two toy 2-D datasets:\n",
        "    - Guassian blobs\n",
        "    - Swiss roll\n",
        "    \n",
        "## Objectives\n",
        "\n",
        "The point of this part of the practical is to get yourself familiar with a GAN model.\n",
        "You are expected to learn technical implementation details of a GAN but also get a better feeling of the challenges of GAN training.\n",
        "As this practical uses simple 2-D data, it will be easy to understand what the model does and what it has learned.\n",
        "\n",
        "## GAN\n",
        "\n",
        "A GAN model introduced by \n",
        "[Goodfellow _et al._ (2014)](https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)\n",
        "consists of two networks trained in adversary, a generator and a discriminator.\n",
        "The generator's goal is to produce realistic samples while the discriminator tries to tell apart the real data samples from the fake samples produced by the generator.\n",
        "The better the generator gets at generating samples, the more difficult discriminator's job is.\n",
        "\n",
        "Note that as GAN contain two separate networks, each trained to be better than the other, the training dynamics can be quite different from standard supervised training. It's not uncommon to observe the losses go up and down during training, as one of the networks get better than the other.\n",
        "\n",
        "## Technical details\n",
        "\n",
        "This Notebook has some `assert`s or checks which you can use to test your implementation of some of the functions. However, keep in mind that passing those checks does not necessarily mean that everything is correct!\n",
        "\n",
        "Some of the `assert`s verify whether shapes of tensors are as expected. Those shapes, however, do not necessarily have to be as expected to have a correct implementation. You are, of course, free to do things differently but make sure you know what you're doing!"
      ],
      "id": "33d35e9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb900d30"
      },
      "source": [
        "---"
      ],
      "id": "eb900d30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eac1b4ee"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools"
      ],
      "id": "eac1b4ee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "109a399d"
      },
      "outputs": [],
      "source": [
        "seed = 42"
      ],
      "id": "109a399d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b0a0e87"
      },
      "source": [
        "# Toy datasets"
      ],
      "id": "0b0a0e87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2b30b31"
      },
      "outputs": [],
      "source": [
        "def dataset_swiss_roll(n_samples, seed):\n",
        "    x, _ = sklearn.datasets.make_swiss_roll(n_samples, noise=0.5, random_state=seed)\n",
        "    x = x[:, [0, 2]]\n",
        "    return x / np.max(np.abs(x)) + np.array([0., 5.])"
      ],
      "id": "d2b30b31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c23b5f8c"
      },
      "outputs": [],
      "source": [
        "def dataset_s_curve(n_samples, seed):\n",
        "    x, _ = sklearn.datasets.make_s_curve(n_samples, noise=0.15, random_state=seed)\n",
        "    x = x[:, [0, 2]]\n",
        "    return x"
      ],
      "id": "c23b5f8c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d1ad0e7"
      },
      "outputs": [],
      "source": [
        "def dataset_blobs(n_samples, seed):\n",
        "    x, _ = sklearn.datasets.make_blobs(\n",
        "        n_samples=n_samples, n_features=2,\n",
        "        centers=8, cluster_std=0.8,\n",
        "        random_state=seed)\n",
        "    return x"
      ],
      "id": "5d1ad0e7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d803339"
      },
      "outputs": [],
      "source": [
        "x = dataset_blobs(n_samples=10000, seed=seed)\n",
        "sns.scatterplot(x=x[:, 0], y=x[:, 1])\n",
        "plt.show()"
      ],
      "id": "2d803339"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61e6297"
      },
      "source": [
        "# Loss functions"
      ],
      "id": "c61e6297"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da8339c0"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optimizers, stax\n",
        "from jax import jit"
      ],
      "id": "da8339c0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0a74d9d"
      },
      "source": [
        "If you have a GPU, make sure you're using it!"
      ],
      "id": "c0a74d9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fe0fa0c"
      },
      "outputs": [],
      "source": [
        "jax.devices()"
      ],
      "id": "2fe0fa0c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29dab03c"
      },
      "source": [
        "### (Task) Implement `sigmoid_cross_entropy` function\n",
        "\n",
        "First, we'll implement a `sigmoid_cross_entropy` function that transforms the input logits with the sigmoid function and computes the cross-entropy loss.\n",
        "\n",
        "The function should compute the following:\n",
        "\n",
        "$$loss(t, l) = - \\left[t \\cdot \\log S(l) + (1 - t) \\cdot \\log(1 - S(l)) \\right]$$\n",
        "\n",
        "where `t` represents the input target value; `l` the logit; and `S(x)` the sigmoid function:\n",
        "$$S(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "Note, that the sigmoid & cross-entropy can be thought of as separate function but here we expect it to be a single function.\n",
        "\n",
        "**Question:**\n",
        "- Is there any (objective) potential benefit of implementing the combination of sigmoid & cross-entropy as a single function?"
      ],
      "id": "29dab03c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b87d3b6"
      },
      "outputs": [],
      "source": [
        "def sigmoid_cross_entropy(*, targets, logits):\n",
        "    assert targets.shape == logits.shape\n",
        "    \n",
        "    raise NotImplementedError('Task: implement!')\n",
        "    loss = NotImplemented\n",
        "\n",
        "    return loss"
      ],
      "id": "4b87d3b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d704c7fe"
      },
      "source": [
        "#### (Task) Test `sigmoid_cross_entropy`"
      ],
      "id": "d704c7fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7268f899"
      },
      "outputs": [],
      "source": [
        "sigmoid_cross_entropy(targets=jnp.array(1.), logits=jnp.array(0.))"
      ],
      "id": "7268f899"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b30dc12"
      },
      "outputs": [],
      "source": [
        "sigmoid_cross_entropy(targets=jnp.array([[1.], [1.]]), logits=jnp.array([[0.], [0.]]))"
      ],
      "id": "4b30dc12"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98adcc67"
      },
      "outputs": [],
      "source": [
        "targets = np.array([0, 1, 0, 1, 1])\n",
        "logits = np.array([0.55, -0.22, 0.8, 0.1, -0.42])\n",
        "returned = sigmoid_cross_entropy(targets=targets, logits=logits)\n",
        "expected_mean_ce_loss = np.array(0.9110423)\n",
        "assert returned.shape == logits.shape, \\\n",
        "    'cross-entropy loss is not expected here to be averaged over samples'\n",
        "np.testing.assert_almost_equal(expected_mean_ce_loss, returned.mean())"
      ],
      "id": "98adcc67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5598095f"
      },
      "source": [
        "### (Optional task) Numerically stable `sigmoid_cross_entropy` loss\n",
        "\n",
        "Feel free to skip this task (or maybe try it at the end after completing everything else?).\n",
        "You should (hopefully) be able to complete the rest of the practical even if your `sigmoid_cross_entropy` function is not able to handle very large or small values, producing NaNs.\n",
        "Note that you typically do not want your model to internally (hidden features, logits, etc.) operate on very large numbers. Such a behavior might indicate some \"issues with training\".\n",
        "However, it does not have to imply any major issues and sometimes might be even expected.\n",
        "\n",
        "Numerical stability of implementation can be important and it is good to be at least familiar with what are the main roots of numerical stability issues and how to address them, what are the common \"tricks\".\n",
        "\n",
        "\n",
        "**(Optional) task:**\n",
        "- Can your function compute the value for $t=0$ and $l=10000.0$ or do you get NaNs?\n",
        "- Do you understand what is the source of the numerical instability (NaNs)?\n",
        "    - Hint: Think of computing $\\log(e^x)$ for $x = 10^9$, for example. The value of the expressions is impossible to calculate by `np.log(np.exp(x))` (returns inf/NaN) but we know that $\\log(e^x)=x$.\n",
        "    \n",
        "- Can you compute by hand the (almost?) exact value of the function for the inputs as above? **Try to transform the formula manually and see if you can implement a more numerically stable `sigmoid_cross_entropy` function that works with very large or small numbers.**\n",
        "\n",
        "- Note that expressions in math are usefully written in the form that is the most readable or intuitive. However, that form might not necessarily be the best to be implemented. Not just because of numerical stability but also in terms of speed or memory usage.\n",
        "\n",
        "- Hopefully, in the future you'd be able to foresee numerical stability issues and address them! The operations like `log` and `exp` you have to be especially careful about!\n"
      ],
      "id": "5598095f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a883133"
      },
      "outputs": [],
      "source": [
        "sigmoid_cross_entropy(targets=jnp.array(0.),\n",
        "                      logits=jnp.array(10000.))"
      ],
      "id": "0a883133"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd894815"
      },
      "source": [
        "### GAN: objectives\n",
        "\n",
        "In this practical, we're going to use the GAN training objectives as introduced by\n",
        "[Goodfellow _et al._ (2014)](https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)\n",
        "(note that $D(x) \\in (0, 1)$):\n",
        "\n",
        "<br>\n",
        "<div align=\"center\">(discriminator)</div>\n",
        "$$\n",
        "-\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}\n",
        "\\left[\n",
        "\\log D(x)\n",
        "\\right]\n",
        "%\n",
        "-\\mathbb{E}_{z \\sim p_z(z)}\n",
        "\\left[\n",
        "\\log (1 - D(G(z)) )\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "<br>\n",
        "<div align=\"center\">(generator: non-saturating objective)</div>\n",
        "$$\n",
        "-\\mathbb{E}_{z \\sim p_z(z)}\n",
        "\\left[\n",
        "\\log D(G(z))\n",
        "\\right].\n",
        "$$\n",
        "\n",
        "Note we use the **non-saturating** loss for the generator. The alternative is, of course, to have the generator optimize just the reverse of the discriminator's objective.\n",
        "In the non-saturating loss, instead of training the generator to minimize the probability of the discriminator being correct, we maximize the probability of the discriminator incorrectly classifying generated samples as true.\n",
        "Therefore, thanks to the formulation of non-saturating loss, the generator can learn (as it gets non-zero gradients) when the discriminator is very strong and correctly classifies generated samples as fake (that is when $log(1-D(G(z))$ would be close to 0).\n",
        "\n",
        "\n",
        "Alternatively, there is a number of modified objectives for training GANs, such as for example:\n",
        "- [WGAN](https://arxiv.org/abs/1701.07875)\n",
        "- [WGAN-GP](https://papers.nips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html)\n",
        "- [LSGAN](https://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)"
      ],
      "id": "fd894815"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5f7b59a"
      },
      "source": [
        "### (Task) Implement discriminator's loss\n",
        "\n",
        "Hint: use `sigmoid_cross_entropy` function"
      ],
      "id": "f5f7b59a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fec8fb7b"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(*, real_logits, fake_logits):\n",
        "    # NOTE: the inputs are expected to be logits, not probabilities!\n",
        "    \n",
        "    raise NotImplementedError('Task: implement!')\n",
        "    loss = NotImplemented\n",
        "    \n",
        "    return loss"
      ],
      "id": "fec8fb7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b3c440c"
      },
      "source": [
        "What result would you expect if both real & fake input logits were equal $0.0$?\n",
        "\n",
        "Try out a couple of values! Does the function return what you would expect it to return?\n",
        "What if you set the real logits high and fake logits low? What if it's vice-versa?"
      ],
      "id": "7b3c440c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb37e223"
      },
      "source": [
        "#### (Task) Test the discriminator loss"
      ],
      "id": "eb37e223"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c511524"
      },
      "outputs": [],
      "source": [
        "discriminator_loss(real_logits=jnp.array([[0.], [0.]]), fake_logits=jnp.array([[0.], [0.]]))"
      ],
      "id": "9c511524"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf1120de"
      },
      "outputs": [],
      "source": [
        "discriminator_loss(real_logits=jnp.array([0., 0.]), fake_logits=jnp.array([0., 0.]))"
      ],
      "id": "cf1120de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f9fbf71"
      },
      "outputs": [],
      "source": [
        "returned = discriminator_loss(\n",
        "    real_logits=jnp.array([0.13, -0.11, -1.12, -0.02]),\n",
        "    fake_logits=jnp.array([-0.88, 0.42, 0.42, -0.81])\n",
        ")\n",
        "assert returned.shape == (), \\\n",
        "        'discriminator loss is expected here to be averaged over samples'\n",
        "expected = 1.5126383\n",
        "np.testing.assert_almost_equal(returned, expected)"
      ],
      "id": "5f9fbf71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1abc011"
      },
      "source": [
        "### (Task) Implement generator loss\n",
        "\n",
        "Hint: Use `sigmoid_cross_entropy` function"
      ],
      "id": "f1abc011"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae012e70"
      },
      "outputs": [],
      "source": [
        "def generator_loss(discriminator_fake_logits):\n",
        "    # NOTE: the inputs are expected to be logits, not probabilities!\n",
        "    \n",
        "    raise NotImplementedError('Task: implement!')\n",
        "    loss = NotImplemented\n",
        "    \n",
        "    return loss"
      ],
      "id": "ae012e70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d511141"
      },
      "source": [
        "#### (Task) Testing the generator loss\n",
        "\n",
        "What result would you expect for the output of the generator $G(z)$ (which is the input to $D(.)$) equal $0.0$?\n",
        "Does the result match your expectation?"
      ],
      "id": "2d511141"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0be4b5f"
      },
      "outputs": [],
      "source": [
        "generator_loss(discriminator_fake_logits=jnp.array([[0.], [0.,]]))"
      ],
      "id": "c0be4b5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d80c500b"
      },
      "outputs": [],
      "source": [
        "generator_loss(discriminator_fake_logits=jnp.array([0., 0.]))"
      ],
      "id": "d80c500b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a4f34ce"
      },
      "outputs": [],
      "source": [
        "returned = generator_loss(discriminator_fake_logits=jnp.array([1.12, -0.12, 0., 0.42, -0.82,]))\n",
        "assert returned.shape == (), \\\n",
        "        'generator loss is expected here to be averaged over samples'\n",
        "expected = 0.68409014\n",
        "np.testing.assert_almost_equal(expected, returned)"
      ],
      "id": "7a4f34ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd04ca54"
      },
      "source": [
        "# Training"
      ],
      "id": "cd04ca54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0faa28dd"
      },
      "outputs": [],
      "source": [
        "from utils import plot_losses, generate_and_plot_samples, plot_logit_space"
      ],
      "id": "0faa28dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1a623de"
      },
      "outputs": [],
      "source": [
        "def iterable_dataset(key, data, batch_size):\n",
        "    n_batches = len(data) // batch_size\n",
        "    x_shuffled = jax.random.permutation(key, data)\n",
        "    dataset = [x_shuffled[i * batch_size: (i + 1) * batch_size] for i in range(n_batches)]\n",
        "    return dataset"
      ],
      "id": "f1a623de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26a8c9b5"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf"
      ],
      "id": "26a8c9b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "280f1193"
      },
      "outputs": [],
      "source": [
        "def create_mlp(n_hid_layers, hid_dim, output_dim):\n",
        "    layers = list(itertools.chain.from_iterable(\n",
        "        [(stax.Dense(hid_dim), stax.Relu) for _ in range(n_hid_layers)]\n",
        "    )) + [stax.Dense(output_dim)]\n",
        "    mpl_init, mpl_apply = stax.serial(*layers)\n",
        "    return mpl_init, mpl_apply"
      ],
      "id": "280f1193"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6fedf5e"
      },
      "source": [
        "## (Task) Implement the `train_step` function\n",
        "\n",
        "Note that the generator and discriminator's parameters should be updated individually.\n",
        "Make sure that the generator's gradients and discriminator's gradients depend on the right \"things\"."
      ],
      "id": "b6fedf5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ef662d"
      },
      "outputs": [],
      "source": [
        "def create_and_train_gan(hparams, data, seed=seed):\n",
        "    data_dim = data[0].shape[-1]\n",
        "    # Create models\n",
        "    \n",
        "    if hparams.model_type == 'mlp':\n",
        "        generator_init, generator_apply = create_mlp(\n",
        "            n_hid_layers=hparams.generator.n_layers,\n",
        "            hid_dim=hparams.generator.hidden_dim,\n",
        "            output_dim=data_dim,\n",
        "        )\n",
        "        discriminator_init, discriminator_apply = create_mlp(\n",
        "            n_hid_layers=hparams.discriminator.n_layers,\n",
        "            hid_dim=hparams.discriminator.hidden_dim,\n",
        "            output_dim=1,\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        raise NotImplementedError(f'model_type: {hparams.model_type}')\n",
        "\n",
        "    # Initialize\n",
        "    key = jax.random.PRNGKey(seed)\n",
        "    key, key_gen, key_dis = jax.random.split(key, 3)\n",
        "    \n",
        "    input_noise_shape = (-1, hparams.input_noise_dim)\n",
        "    data_shape = (-1, data_dim)\n",
        "    _, gen_params = generator_init(key_gen, input_noise_shape)\n",
        "    _, dis_params = discriminator_init(key_dis, data_shape)\n",
        "\n",
        "    ## Initialize: optimizers\n",
        "    gen_opt_init, gen_opt_update, gen_get_params = optimizers.adam(\n",
        "        step_size=hparams.gen_lr, b1=hparams.beta1)\n",
        "    dis_opt_init, dis_opt_update, dis_get_params = optimizers.adam(\n",
        "        step_size=hparams.dis_lr, b1=hparams.beta1)\n",
        "    gen_opt_state = gen_opt_init(gen_params)\n",
        "    dis_opt_state = dis_opt_init(dis_params)\n",
        "    \n",
        "    # NOTE: can comment out @jax.jit for debugging but\n",
        "    #       make sure to use it while training, as it will be much faster\n",
        "    # HINT: you are expected to define functions that contain forward passes\n",
        "    @jax.jit\n",
        "    def train_step(step, gen_opt_state, dis_opt_state, noise, real_samples):\n",
        "        gen_params = gen_get_params(gen_opt_state)\n",
        "        dis_params = dis_get_params(dis_opt_state)\n",
        "        \n",
        "        raise NotImplementedError('Task: implement!')\n",
        "        gen_grads = NotImplemented\n",
        "        dis_grads = NotImplemented\n",
        "        gen_loss = NotImplemented\n",
        "        dis_loss = NotImplemented\n",
        "\n",
        "        gen_opt_state = gen_opt_update(step, gen_grads, gen_opt_state)\n",
        "        dis_opt_state = dis_opt_update(step, dis_grads, dis_opt_state)\n",
        "        return (gen_loss, dis_loss), (gen_opt_state, dis_opt_state)\n",
        "    \n",
        "    # Training\n",
        "    losses = {'gen': [], 'dis': []}\n",
        "\n",
        "    total_step = 0\n",
        "    for epoch in range(hparams.epochs):\n",
        "        key, key_data = jax.random.split(key)\n",
        "        \n",
        "        for batch_x in iterable_dataset(key=key_data, data=data, batch_size=hparams.batch_size):\n",
        "            \n",
        "            key, subkey = jax.random.split(key)\n",
        "            noise = jax.random.normal(subkey, (hparams.batch_size, hparams.input_noise_dim))\n",
        "            \n",
        "            (gen_loss, dis_loss), (gen_opt_state, dis_opt_state) = train_step(\n",
        "                step=total_step,\n",
        "                gen_opt_state=gen_opt_state,\n",
        "                dis_opt_state=dis_opt_state,\n",
        "                noise=noise,\n",
        "                real_samples=batch_x,\n",
        "            )\n",
        "            losses['gen'].append(gen_loss)\n",
        "            losses['dis'].append(dis_loss)\n",
        "            \n",
        "            total_step += 1\n",
        "            \n",
        "        if epoch == 0 or (epoch < 100 and epoch % 10 == 9) or epoch % 100 == 99:\n",
        "            print('-' * 30, 'epoch', epoch, '-' * 30)\n",
        "            fig, axes = plt.subplots(\n",
        "                1,3,\n",
        "                figsize=(17,5))\n",
        "            plot_losses(losses,axes[0])\n",
        "            key, subkey = jax.random.split(key)\n",
        "            samples = generate_and_plot_samples(\n",
        "                subkey, \n",
        "                data,\n",
        "                generator_apply, \n",
        "                gen_get_params(gen_opt_state),\n",
        "                discriminator_apply,\n",
        "                gen_get_params(dis_opt_state),\n",
        "                n_samples=256, \n",
        "                input_noise_dim=hparams.input_noise_dim,\n",
        "                ax=axes[1])\n",
        "            plot_logit_space(dis_get_params(dis_opt_state),\n",
        "                             discriminator_apply,\n",
        "                             samples,\n",
        "                             ax=axes[2],\n",
        "                             ref_ax=axes[1],\n",
        "                             apply_axis=[0,1])\n",
        "            plt.show()"
      ],
      "id": "30ef662d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fab48edc"
      },
      "source": [
        "## (Task) Train the GAN model: simple blobs data\n",
        "\n",
        "Below you can the output of my training. Can you get similar results?\n",
        "Note that the output you get might not be exactly the same as the one below but given that the code skeleton already covers most of the details that could matter, it should be fairly similar.\n",
        "\n",
        "write a few sentences about each of the following discussion points. \n",
        "**Discussion points:**\n",
        "* Is the generator able to cover all of the blobs?\n",
        "* Are the individual blobs well covered by the generated samples?\n",
        "* Can you observe mode collapse?"
      ],
      "id": "fab48edc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96777ffa"
      },
      "outputs": [],
      "source": [
        "hparams = OmegaConf.create({\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 256,\n",
        "    'gen_lr': 0.0001,\n",
        "    'dis_lr': 0.0001,\n",
        "    'beta1': 0.5,  # NOTE: important!\n",
        "    'model_type': 'mlp',\n",
        "    'generator': {\n",
        "        'n_layers': 5,\n",
        "        'hidden_dim': 512,\n",
        "    },\n",
        "    'discriminator': {\n",
        "        'n_layers': 4,\n",
        "        'hidden_dim': 512,\n",
        "    },\n",
        "    'input_noise_dim': 2,\n",
        "})\n",
        "create_and_train_gan(hparams, dataset_blobs(n_samples=10000, seed=seed), seed=seed)"
      ],
      "id": "96777ffa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9017faf1"
      },
      "outputs": [],
      "source": [
        "hparams = OmegaConf.create({\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 256,\n",
        "    'gen_lr': 0.0001,\n",
        "    'dis_lr': 0.0001,\n",
        "    'beta1': 0.5,  # NOTE: important!\n",
        "    'model_type': 'mlp',\n",
        "    'generator': {\n",
        "        'n_layers': 6,\n",
        "        'hidden_dim': 512,\n",
        "    },\n",
        "    'discriminator': {\n",
        "        'n_layers': 5,\n",
        "        'hidden_dim': 512,\n",
        "    },\n",
        "    'input_noise_dim': 2,\n",
        "})\n",
        "create_and_train_gan(hparams, dataset_blobs(n_samples=10000, seed=seed), seed=seed)"
      ],
      "id": "9017faf1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74d2aed6"
      },
      "source": [
        "If you get a similar result feel free to play with the hyperparameter values!\n",
        "Which of them seem important? Which of them the model is not so sensitive?\n",
        "\n",
        "You can also try to change the random seed and see if the results change significantly. Do the results seem more consistent across different seeds for different hyperparamer settings?"
      ],
      "id": "74d2aed6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d8502a4"
      },
      "source": [
        "## (Optional Task) Train the GAN model: Swiss roll data\n",
        "\n",
        "Now, let's try a slightly difficult problem!"
      ],
      "id": "8d8502a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b66dc490"
      },
      "outputs": [],
      "source": [
        "hparams = OmegaConf.create({\n",
        "    'epochs': 1000,\n",
        "    'batch_size': 256,\n",
        "    'gen_lr': 0.0001,\n",
        "    'dis_lr': 0.0001,\n",
        "    'beta1': 0.5,  # NOTE: important!\n",
        "    'model_type': 'mlp',\n",
        "    'generator': {\n",
        "        'n_layers': 6,\n",
        "        'hidden_dim': 512,\n",
        "    },\n",
        "    'discriminator': {\n",
        "        'n_layers': 5,\n",
        "        'hidden_dim': 512,\n",
        "    },\n",
        "    'input_noise_dim': 2,\n",
        "})\n",
        "\n",
        "create_and_train_gan(hparams, dataset_swiss_roll(n_samples=10000, seed=seed), seed=seed)"
      ],
      "id": "b66dc490"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c0507c6"
      },
      "source": [
        "As you see the model (with the original hyperparameter values) is not really able to model the data well :/\n",
        "\n",
        "Are you able to improve the model to get a results like the following figure?\n",
        "\n",
        "![Swiss roll](./imgs/step1_swiss_roll.png)\n",
        "\n",
        "**Here are some sugestions what to try:**\n",
        "- Different hparams, architecture\n",
        "- Updating the discriminator more often than the generator (e.g. 5 updates)\n",
        "- Different initialization of weights (e.g. Normal with std = 0.02)\n",
        "- Different GAN objectives ([WGAN](https://arxiv.org/abs/1701.07875),\n",
        "[WGAN-GP](https://papers.nips.cc/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html),\n",
        "[LSGAN](https://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf))\n",
        "\n",
        "Note that these are just suggestions! The point of them is to make you try out different things yourself. Some of them should help significantly, while others might not help much or even make things worse! You need to try them yourself to find out!"
      ],
      "id": "4c0507c6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}