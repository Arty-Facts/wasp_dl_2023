{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vsctzOG586n-"
      },
      "source": [
        "# Uncertainty Estimation with Probabilistic Discriminative Deep Networks\n",
        "\n",
        "In this implementation exercise, we understand the notions of aleatoric and epistemic uncertainties and practice modelling such uncertainties for simple deep networks.\n",
        "\n",
        "## Introduction\n",
        "In standard discriminative modelling, we try to learn the parameters $\\mathbf{\\theta}$ of a deep network $f_{\\mathbf{\\theta}}:\\mathcal{X}\\rightarrow \\mathcal{Y}$ that maps an input $x\\in\\mathcal{X}=\\mathbb{R}^d$ to its corresponding output $\\hat{y}=f_{\\mathbf{\\theta}}(\\mathbf{x})\\in\\mathcal{Y}$. The parameters $\\theta \\in \\mathbb{R}^p$ are trained on a training set of $n$ samples $\\mathcal{D}=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$\n",
        "\n",
        "Uncertainty estimation is concerned with reflecting a *degree of confidence* with a deep network's prediction. That is, instead of directly mapping the input $\\mathbf{x}$ to the corresponding prediction $\\hat{y}$, it maps it to a distribution over possible outputs $\\hat{P}(y|\\mathbf{x})$.\n",
        "\n",
        "In this implementation practical, we first create a toy dataset and then try simple techniques to model two main types of uncertainties: *aleatoric* and *epistemic*.\n",
        "\n",
        "## Types of Uncertainty \n",
        "**Aleatoric uncertainty**, also known as irreducable uncertainty, refers to the inherent uncertainty that are present in the observations $(\\mathbf{x},y)$. For instance, the measurements (*e.g.,* length of a tool) can be noisy depending on the precision of the measuring device (*e.g.*, a ruler with one-centimeter markings). This type of uncertainty cannot be remedied by better modelling or additional data, hence the adjective \"irreducible\". Aleatoric uncertainty can be either *homoscedastic* and *heteroscedastic*. Homoscedastic aleatoric uncertainty does not depend on a sample $\\mathbf{x}$ (*e.g.*, the length of a tool can always have a fixed error of up to one centimeter) while a heteroscedastic aleatoric uncertainty can change depending on the sample (*e.g.*, a depth camera might give more error on certain surfaces than others depending on the material of the surface and reflectance and illuminance properties).  \n",
        "\n",
        "**Epistemic uncertainty**, also known as knowledge uncertainty,  is induced by a model's lack of (enough) knowledge about a certain sample to make a confident prediction on it. As such, epistemic uncertainty is always heteroscedastic *i.e.*, sample-dependent. Knowledge uncertainty is sometimes further divided into seprate *model* and *distributional* uncertainties reflecting the uncertainty induced by the inability of modeling and lack of enough data respectively. As such, epistemic uncertainty can be tackled by either improving the modelling or increasing the dataset size.\n",
        "\n",
        "We will use a few simple methods, described in the lectures, to tackle these uncertainties by using \n",
        "\n",
        "i) Maximum Likelihood Estimation (MLE) of aleatoric uncertainty, and \n",
        "\n",
        "ii) Variational Inference (VI)-based Bayesian deep networks for epistemic uncertainty.\n",
        "\n",
        "But let's first create a toy dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CMa4a3Eqb4D8"
      },
      "outputs": [],
      "source": [
        "#@title Import the necessary libraries for creating the data (numpy) and plotting it\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cWdtg1fGcFKp"
      },
      "source": [
        "# Toy Regression Dataset Creation \n",
        "\n",
        "## Introduction\n",
        "In this exercise the goal is to familiarize the students with basic concepts of deep probabilistic modelling. As such, we create and use a toy dataset since it will 1) allow us to put more emphasis on the pedagogical aspect of the practical, 2) it will make visualizing the important concepts easier, and 3) reduce the computational load necessary for the analysis.\n",
        "\n",
        "## Underlying Function\n",
        "For this first toy dataset, we consider a one-dimensional regression task $\\mathcal{X}=\\mathbb{R}\\rightarrow\\mathcal{Y}=\\mathbb{R}$. The underlying true function is the periodic even cosine function $y=\\text{cos}(x)$. Modelling a periodic function can be generally challenging for a deep network. \n",
        "\n",
        "## Basic Clean Training Set\n",
        "For the basic clean training set $\\mathcal{D}$, we create $n=100$ samples with $x\\in[-1.3\\pi,1.3\\pi]$ and store them in the following variables.\n",
        "\n",
        "```\n",
        "# X_train       # training points input\n",
        "# Y_train_true  # training points clean label\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCElQyBFBxuh"
      },
      "outputs": [],
      "source": [
        "#@title define a function to sample from the true distribution\n",
        "def sample_cosine(n_samples,\n",
        "                  domain,\n",
        "                  gaussian_noise_std=None,\n",
        "                  heteroscedastic_noise_fun=None):\n",
        "    assert gaussian_noise_std is None or heteroscedastic_noise_fun is None,\\\n",
        "        \"Either gaussian_noise_std and heteroscedastic_noise_fun can be provided\"\n",
        "    # we assume that the samples are uniformly distributed\n",
        "    x = np.linspace(domain[0],\n",
        "                    domain[1],\n",
        "                    n_samples).reshape(-1, 1)\n",
        "\n",
        "    if gaussian_noise_std is None and heteroscedastic_noise_fun is None:\n",
        "        y = np.cos(x)\n",
        "    else:\n",
        "        gaussian_noise = np.random.randn(n_samples, 1)\n",
        "        if heteroscedastic_noise_fun is None:\n",
        "            y = np.cos(x) + gaussian_noise * gaussian_noise_std\n",
        "        else:\n",
        "            y = np.cos(x)\n",
        "            y = y + heteroscedastic_noise_fun(gaussian_noise,x,y)\n",
        "        \n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhBHlFTifxch"
      },
      "outputs": [],
      "source": [
        "n_train = 100\n",
        "domain_train= np.pi * np.array([-1.3, 1.3])\n",
        "X_train, Y_train_true = sample_cosine(n_train,\n",
        "                                      domain_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPoNfZdgfgK"
      },
      "source": [
        "let's visually inspect our clean training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtls-rV-b_4g"
      },
      "outputs": [],
      "source": [
        "#@title Define a function for visualization\n",
        "def visualize_samples(samples_list,ux,uy,domain_visualization,title, figsize=(15, 4)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    for x_samples,y_samples,kwargs in samples_list:\n",
        "        plt.scatter(x_samples, y_samples,**kwargs)\n",
        "\n",
        "    plt.plot(ux, uy, linewidth=3, label='Underlying Function')\n",
        "    plt.xlim(domain_visualization)\n",
        "    plt.title(title)\n",
        "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MJq9lc-fPva"
      },
      "outputs": [],
      "source": [
        "#@title Sample at a higher resolution for visualization of the true function\n",
        "domain_visualization = np.pi * np.array([-3.2, 3.2])\n",
        "visualization_resolution = 1000\n",
        "ux, uy = sample_cosine(visualization_resolution,\n",
        "                       domain_visualization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQAB0CiGgk52"
      },
      "outputs": [],
      "source": [
        "#@title Visualize training data\n",
        "visualization_train_kwargs = {'marker':'X',\n",
        "                              'linewidth':1,\n",
        "                              'color':'darkgreen', \n",
        "                              'label':'Clean Training data'}\n",
        "visualize_samples_list = [(X_train,\n",
        "                           Y_train_true,\n",
        "                           visualization_train_kwargs)]\n",
        "zooming_factor=1.2\n",
        "visualize_samples(visualize_samples_list,\n",
        "                  ux,\n",
        "                  uy,\n",
        "                  domain_visualization/zooming_factor,\n",
        "                  title='Clean training data and the underlying function')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RC_q2wBYg_CZ"
      },
      "source": [
        "so, the training data densely covers the interval $-1.3\\pi, 1.3\\pi$.\n",
        "\n",
        "## Basic Clean Test Set\n",
        "Now let's create a test set that covers inputs beyond the observed interval during training. For the basic clean test set $\\mathcal{D}_{test}$, we create $n_{test}=200$ samples with $x\\in[-3\\pi,3\\pi]$ and store them in the following variables.\n",
        "\n",
        "```\n",
        "# X_test       # test points input\n",
        "# Y_test_true  # test points clean label\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_ereumguECr"
      },
      "outputs": [],
      "source": [
        "n_test = 200\n",
        "domain_test = np.pi * np.array([-3, 3])\n",
        "X_test, Y_test_true = sample_cosine(n_test,\n",
        "                                    domain_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc2vPT3EvKd1"
      },
      "source": [
        "let's visually inspect our clean test and training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX661jUgvEhU"
      },
      "outputs": [],
      "source": [
        "visualization_test_kwargs={'marker':'X',\n",
        "                           'linewidth':1, \n",
        "                           'color':'red', \n",
        "                           'label':'Clean Testing data'}\n",
        "# update visualization list\n",
        "visualize_samples_list.append(\n",
        "    (X_test,\n",
        "     Y_test_true,\n",
        "     visualization_test_kwargs))\n",
        "visualize_samples(visualize_samples_list,\n",
        "                  ux,\n",
        "                  uy, \n",
        "                  domain_visualization,\n",
        "                  title='Clean training data, test data, and the underlying function')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GivKQnrhwTVk"
      },
      "source": [
        "## Homoscedastic Gaussian Noise on the Training Set\n",
        "Datasets are commonly noisy due to various reasons, for instance because of the measurement errors. So, a more realistic dataset will contain some noise in the labels. We will first create a dataset containing *homoscedastic* label noise -- a noise independent of the sample. For this, we choose an additive Gaussian noise $\\mathcal{N}(0,\\sigma^2)$ with variance $\\sigma^2=0.01$. We call the dataset with the homoscedastic noise $\\mathcal{D}^{homo}$. It contains the same input samples as that of $\\mathcal{D}$ but the corresponding labels $y$ are generated with additive gaussian noise and stored in the following variable.\n",
        "\n",
        "```\n",
        "# Y_train_homo  # training points label with homoscedastic additive Gaussiannoise\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55YGHhR_0zra"
      },
      "outputs": [],
      "source": [
        "noise_std = 0.1\n",
        "# sample again with noise\n",
        "X_train, Y_train_hom = sample_cosine(n_train,\n",
        "                                     domain_train,\n",
        "                                     gaussian_noise_std=noise_std)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6A26VD9r2VKB"
      },
      "source": [
        "Now, let us visualize the training data with homoscedastic label noise this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhZI6szg1-Wg"
      },
      "outputs": [],
      "source": [
        "# update visualization information\n",
        "visualization_train_kwargs['label']='Training data with homoscedastic noise'\n",
        "visualize_samples_list[0]=(X_train,\n",
        "                           Y_train_hom,\n",
        "                           visualization_train_kwargs)\n",
        "visualize_samples(visualize_samples_list,\n",
        "                  ux,\n",
        "                  uy, \n",
        "                  domain_visualization,\n",
        "                  title='Noisy training data, test data and the underlying function')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AHrhDm2E2xoq"
      },
      "source": [
        "## Heteroscedastic Noise on the Training Set\n",
        "We then create another dataset, but this time, containing *heteroscedastic* label noise -- a noise that is dependent on the sample. It is common for the label noise to be dependent on the magnitude for continuous labels. For this reason, we choose a *multiplicative* Gaussian noise $\\mathcal{N}(\\mu,\\sigma^2)$ with mean $\\mu=1$ and variance $\\sigma^2=0.04$. We call the dataset with the heteroscedastic noise $\\mathcal{D}^{het}$. It contains the same input samples as that of $\\mathcal{D}$ but the corresponding labels $y$ are generated with multiplicative gaussian noise and stored in the following variable.\n",
        "\n",
        "```\n",
        "# Y_train_het  # training points label with homoscedastic additive Gaussiannoise\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH4QrK-54uUe"
      },
      "outputs": [],
      "source": [
        "noise_std = 0.2\n",
        "noise_mean = 1\n",
        "noise_fun = lambda noise,x,y: (noise_std * noise + noise_mean - 1) * y\n",
        "X_train, Y_train_het = sample_cosine(n_train,\n",
        "                                     domain_train,\n",
        "                                     heteroscedastic_noise_fun=noise_fun)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "je32Rldf5R1P"
      },
      "source": [
        "Now, let us visualize the training data with heteroscedastic label noise this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoRfto0v5VhG"
      },
      "outputs": [],
      "source": [
        "# update visualization information\n",
        "visualization_train_kwargs['label']='Training data with heteroscedastic noise'\n",
        "visualize_samples_list[0]=(X_train,\n",
        "                           Y_train_het,\n",
        "                           visualization_train_kwargs)\n",
        "visualize_samples(visualize_samples_list,\n",
        "                  ux,\n",
        "                  uy, \n",
        "                  domain_visualization,\n",
        "                  title='Noisy training data, test data and the underlying function')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nAXN1sRT7tW2"
      },
      "source": [
        "# Regularized Risk Minimization\n",
        "\n",
        "In a regularized risk minimization scheme, we train a model parametrized by $\\mathbf{\\theta}$, on a training set $\\mathcal{D}=\\{(x_i,y_i)\\}_{i=1}^{n}$, using the following minimization:\n",
        "\\begin{align}\n",
        "\\mathbf{\\theta}^*=\\text{argmin}_{\\mathbf{\\theta}} \\mathcal{L}(\\mathcal{D}) + \\Omega(\\mathbf{\\theta}) = \\text{argmin}_{\\mathbf{\\theta}}\\sum_{i=1}^{n}l(f_\\theta(x_i), y_i) + \\Omega(\\mathbf{\\theta}), \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathcal{L}$ is the total loss function operating on the who;e training set, $l$ is the sample loss function operating on individual training pairs $(x_i,y_i)$ and $\\Omega$ is a regularization cost function.\n",
        "\n",
        "## Training a Multi-Layer Perceptron (MLP)\n",
        "Initially, we train a MLP, also known as fully-connected network, parametrized by its weights and biases $\\mathbf{\\theta}$. We use three hidden layers of size 100,40,100 units and the Rectified Linear Unit (ReLU), $\\max(0,h)$, as the activation function. We adopt mean squared error as the loss function and weight decay as the regularization technique. Throughout this practical, we use Jax as the framework to implement our deep networks.\n",
        "\n",
        "### Jax Deep Learning Framework\n",
        "Jax uses a standard python (Numpy) implementation for describing a model's operations and enables GPU-optimized and differentible training using XLA and Autograd respectively. A simple tutorial can be found on the project page: https://github.com/google/jax. \n",
        "\n",
        "Let us first import the necessary Jax libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iwLfu-mqwC8I"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, value_and_grad\n",
        "from jax import random\n",
        "from jax import nn\n",
        "from jax.example_libraries import optimizers\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "# initialize the random generator \n",
        "key = random.PRNGKey(1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iu_tn58wASFZ"
      },
      "source": [
        "Next, we define the configuration of the fully-connected (FC) network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsH6EdhyAhGG"
      },
      "outputs": [],
      "source": [
        "# standard network's configurations\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = Y_train_true.shape[1]\n",
        "layers_width = [100, 40, 100]\n",
        "num_epochs = 1000\n",
        "step_size = 1e-2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e0f6P3odBBya"
      },
      "source": [
        "Now, we define two python functions; one for initializing the parameters of a FC *layer* and another for the feed-forward computation of a FC *layer*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHpCW2ofA7Lf"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters of a standard fully-connected layer \n",
        "def standard_fc_layer_init(input_dim, output_dim, key, scale=1e-2):\n",
        "    keys = random.split(key, 2)\n",
        "    weights = random.normal(keys[0], (output_dim, input_dim)) * scale\n",
        "    biases  = random.normal(keys[1], (output_dim, 1)) * scale\n",
        "    return weights, biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wCKQCSLBVky"
      },
      "outputs": [],
      "source": [
        "# Forward pass of a standard fully-connected layer \n",
        "def standard_fc_layer_forward(input, weights, biases):\n",
        "    return jnp.dot(weights, input) + biases"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EkEex5qZBxTb"
      },
      "source": [
        "Next, we define two python functions; one for initializing the parameters of a FC *network* and another for the feed-forward computation of a FC *network*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POX0PdQmCAGG"
      },
      "outputs": [],
      "source": [
        "# Initialize A standard fully-connected network \n",
        "def standard_fc_network_init(input_dim, layers_width, output_dim, key):\n",
        "    network_feature_dims = [input_dim, *layers_width, output_dim]\n",
        "    number_of_layers = len(network_feature_dims[:-1])\n",
        "    keys = random.split(key, number_of_layers)\n",
        "    params = [standard_fc_layer_init(network_feature_dims[i],\n",
        "                                    network_feature_dims[i+1],\n",
        "                                    keys[i]) \n",
        "                for i in range(number_of_layers)]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VQJ7ReVCAfG"
      },
      "outputs": [],
      "source": [
        "# Forward pass of a standard fully-connected network \n",
        "def standard_fc_network_forward(input, params, is_classification=False):\n",
        "    representation = input\n",
        "    for weights,biases in params[:-1]:\n",
        "        representation = nn.relu(standard_fc_layer_forward(representation, weights, biases))\n",
        "    representation = standard_fc_layer_forward(representation,params[-1][0], params[-1][1])\n",
        "\n",
        "    if is_classification:\n",
        "        logits = representation - nn.logsumexp(representation)\n",
        "        return logits\n",
        "    \n",
        "    return representation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6uPzDOOlCTym"
      },
      "source": [
        "In the following, we define a python function corresponding to the mean squared error (MSE) loss, as \n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\mathcal{D})=\\sum_{i=1}^{n}l(f_\\theta(x_i),y_i)=\\sum_{i=1}^{n}(f_\\theta(x_i)-y_i)^2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms_Ity8gCSQS"
      },
      "outputs": [],
      "source": [
        "def loss_mse(params, X, Y):\n",
        "    \"\"\" Compute the MSE loss \"\"\"\n",
        "    preds = standard_fc_network_forward(X, params)\n",
        "    return jnp.mean(jnp.power(preds[0,:] - Y, 2))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahc5a-oDE5sw"
      },
      "source": [
        "Finally, we implement the code necessary for optimizing the network's parameters using the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL6_JYJAFBhT"
      },
      "outputs": [],
      "source": [
        "# Compute the gradient for a batch and update the parameters \n",
        "@partial(jit, static_argnums=(4,))\n",
        "def update(params, X, Y, opt_state, loss):\n",
        "    value, grads = value_and_grad(loss)(params, X, Y)\n",
        "    opt_state = opt_update(0, grads, opt_state)\n",
        "    return get_params(opt_state), opt_state, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0xl2tKsFaE-"
      },
      "outputs": [],
      "source": [
        "# Implements a learning loop over epochs. \n",
        "def run_training_loop(num_epochs, opt_state, X, Y, X_test, Y_test, num_layers, loss):\n",
        "    # Initialize placeholder for logging\n",
        "    train_loss, train_mse, test_mse = [], [], []\n",
        "\n",
        "    # Get the initial set of parameters\n",
        "    params = get_params(opt_state)\n",
        "\n",
        "    # Get initial loss after random init\n",
        "    train_loss.append(loss(params, X.T, Y.T))\n",
        "    train_mse.append(loss_mse(params[:num_layers], X.T, Y.T))\n",
        "    test_mse.append(loss_mse(params[:num_layers], X_test.T, Y_test.T))\n",
        "    print('Starting loss: ', train_loss[-1], ' Starting mse: ', train_mse[-1])\n",
        "\n",
        "    # Loop over the training epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        params, opt_state, loss_val = update(params, X.T, Y.T, opt_state, loss)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        test_mse.append(loss_mse(params[:num_layers], X_test.T, Y_test.T))\n",
        "        train_loss.append(loss(params, X.T, Y.T))\n",
        "        train_mse.append(loss_mse(params[:num_layers], X.T, Y.T))\n",
        "        if epoch%200 == 0:\n",
        "            print(\"Epoch {} | T: {:0.6f} | Train loss:\"\n",
        "                  \" {:0.3f} | Train mse: {:0.3f} | Test mse:\"\n",
        "                  \" {:0.3f}\".format(epoch+1, \n",
        "                                    epoch_time,\n",
        "                                    train_loss[-1], \n",
        "                                    train_mse[-1], \n",
        "                                    test_mse[-1]))\n",
        "\n",
        "    return params, train_loss, train_mse, test_mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTy9v5EZ5y2h"
      },
      "outputs": [],
      "source": [
        "def visulize_predictions(Y_test_pred, title_text, X_train=[], Y_train=[], sigma_param=-1):\n",
        "    fig = plt.figure(figsize=(15, 4))\n",
        "    ux = np.pi * np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
        "    uy = np.cos(ux)\n",
        "    plt.plot(ux, uy, linewidth=3, label='Underlying Function')\n",
        "    plt.plot(X_test, Y_test_pred.T, linewidth=3, color='red', label='Prediction on Testing Points')\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([-1.5,1.5])\n",
        "    if not jnp.isscalar(sigma_param) or sigma_param >= 0:\n",
        "        plt.fill_between(X_test.ravel(), \n",
        "                        (Y_test_pred + 2 * sigma_param).ravel(), \n",
        "                        (Y_test_pred - 2 * sigma_param).ravel(), \n",
        "                        alpha=0.3, label='Uncertainty')\n",
        "        plt.scatter(X_train, \n",
        "                    Y_train, \n",
        "                    marker='X', \n",
        "                    linewidth=1, \n",
        "                    color='darkgreen',\n",
        "                    label='Clean Training data')\n",
        "    plt.title(title_text)\n",
        "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
        "    plt.grid()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_MmEKg6QF0OO"
      },
      "source": [
        "Finally, let us launch the training for the clean dataset and visualize the learnt function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYJTOyFTF42J",
        "outputId": "49e3f943-4175-4ce9-a847-3f7cdba053b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting loss:  0.5547652  Starting mse:  0.5547652\n",
            "Epoch 1 | T: 0.782735 | Train loss: 0.548 | Train mse: 0.548 | Test mse: 0.503\n",
            "Epoch 201 | T: 0.000593 | Train loss: 0.000 | Train mse: 0.000 | Test mse: 0.635\n",
            "Epoch 401 | T: 0.000872 | Train loss: 0.000 | Train mse: 0.000 | Test mse: 0.970\n",
            "Epoch 601 | T: 0.000574 | Train loss: 0.000 | Train mse: 0.000 | Test mse: 1.106\n",
            "Epoch 801 | T: 0.000597 | Train loss: 0.000 | Train mse: 0.000 | Test mse: 1.185\n"
          ]
        }
      ],
      "source": [
        "# Initialize the network parameters\n",
        "params = standard_fc_network_init(input_dim,\n",
        "                                  layers_width,\n",
        "                                  output_dim,\n",
        "                                  key)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "params,\\\n",
        "train_loss,\\\n",
        "train_mse,\\\n",
        "test_mse = run_training_loop(num_epochs, \n",
        "                             opt_state, \n",
        "                             X_train,\n",
        "                             Y_train_true,\n",
        "                             X_test, \n",
        "                             Y_test_true,\n",
        "                             len(layers_width)+1, \n",
        "                             loss_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irHjOYzT54ep"
      },
      "outputs": [],
      "source": [
        "Y_test_pred = standard_fc_network_forward(X_test.T, params)\n",
        "visulize_predictions(Y_test_pred, \n",
        "                     r'Predictions of a $\\bf{Standard\\, FC\\, network}$'\n",
        "                     r' trained on $\\bf{clean\\, training\\, data}$')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JXOlUIvDIC1D"
      },
      "source": [
        "We get a great match in the training range $[-1.3\\pi,1.3\\pi]$ while the predictions become poor outside this range which is understandable due to the hardness of generalizing to periodic functions after only observing one period and in the absence of helpful inductive biases.\n",
        "\n",
        "Now, let's train and test on the training data with homoscedastic noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTVPvOwhIB0W",
        "outputId": "5bc9d306-cd54-47a6-dd40-dde2be5a4ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting loss:  0.5725889  Starting mse:  0.5725889\n",
            "Epoch 1 | T: 0.001053 | Train loss: 0.566 | Train mse: 0.566 | Test mse: 0.503\n",
            "Epoch 201 | T: 0.000962 | Train loss: 0.007 | Train mse: 0.007 | Test mse: 0.352\n",
            "Epoch 401 | T: 0.000645 | Train loss: 0.007 | Train mse: 0.007 | Test mse: 0.353\n",
            "Epoch 601 | T: 0.000638 | Train loss: 0.007 | Train mse: 0.007 | Test mse: 0.350\n",
            "Epoch 801 | T: 0.000631 | Train loss: 0.007 | Train mse: 0.007 | Test mse: 0.354\n"
          ]
        }
      ],
      "source": [
        "# Initialize the network parameters\n",
        "params_hom = standard_fc_network_init(input_dim,\n",
        "                                      layers_width,\n",
        "                                      output_dim, \n",
        "                                      key)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params_hom)\n",
        "\n",
        "params_hom,\\\n",
        "train_loss_hom,\\\n",
        "train_mse_hom,\\\n",
        "test_mse_hom = run_training_loop(num_epochs, \n",
        "                                 opt_state, \n",
        "                                 X_train, \n",
        "                                 Y_train_hom,\n",
        "                                 X_test, \n",
        "                                 Y_test_true, \n",
        "                                 len(layers_width)+1, \n",
        "                                 loss_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzmbMlqX6oTM"
      },
      "outputs": [],
      "source": [
        "Y_test_pred_hom = standard_fc_network_forward(X_test.T, params_hom)\n",
        "visulize_predictions(Y_test_pred_hom, \n",
        "                     r'Predictions of a $\\bf{Standard\\, FC\\, network}$'\n",
        "                     r' trained on training data with $\\bf{homoscedastic\\, noise}$')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3_p9JVcjKUgH"
      },
      "source": [
        "We can see that the network overfits to some of the noise present in the training data.\n",
        "\n",
        "Let us go to the dataset exhibiting heteroscedastic label noise next: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH4x3U49KU8-",
        "outputId": "94060b14-5683-4ccd-fe7a-c7a886212cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting loss:  0.56727725  Starting mse:  0.56727725\n",
            "Epoch 1 | T: 0.000849 | Train loss: 0.560 | Train mse: 0.560 | Test mse: 0.503\n",
            "Epoch 201 | T: 0.000649 | Train loss: 0.022 | Train mse: 0.022 | Test mse: 0.483\n",
            "Epoch 401 | T: 0.000578 | Train loss: 0.019 | Train mse: 0.019 | Test mse: 0.550\n",
            "Epoch 601 | T: 0.000574 | Train loss: 0.019 | Train mse: 0.019 | Test mse: 0.549\n",
            "Epoch 801 | T: 0.000683 | Train loss: 0.017 | Train mse: 0.017 | Test mse: 0.662\n"
          ]
        }
      ],
      "source": [
        "# Initialize the network parameters\n",
        "params_het = standard_fc_network_init(input_dim, \n",
        "                                      layers_width, \n",
        "                                      output_dim, \n",
        "                                      key)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params_het)\n",
        "\n",
        "params_het,\\\n",
        "train_loss_het,\\\n",
        "train_mse_het,\\\n",
        "test_mse_het = run_training_loop(num_epochs, \n",
        "                                 opt_state, \n",
        "                                 X_train, \n",
        "                                 Y_train_het, \n",
        "                                 X_test, \n",
        "                                 Y_test_true, \n",
        "                                 len(layers_width)+1, \n",
        "                                 loss_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx7fblr962Zt"
      },
      "outputs": [],
      "source": [
        "Y_test_pred_het = standard_fc_network_forward(X_test.T, \n",
        "                                              params_het)\n",
        "visulize_predictions(Y_test_pred_het,\n",
        "                     r'Predictions of a $\\bf{Standard\\, FC\\, network}$'\n",
        "                     r' trained on training data with $\\bf{heteroscedastic\\, noise}$')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_hvlp0_sNGoW"
      },
      "source": [
        "Here we see that the network overfits a bit more to the heteroscedastic noise present in the training data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oQVuHLCiMoOs"
      },
      "source": [
        "# Training with Assuming a Label Noise Distribution\n",
        "\n",
        "In the class, we discussed that one way to model aleatoric uncertainty is using maximum likelihood estimation (MLE) in tandem with a model that can output probability distributions. Here, we are going to follow this idea to see how well this approach can predict the aleatoric uncertainty.\n",
        "\n",
        "## Modelling Aleatoric Uncertainty: Learning to predict a fixed (homoscedastic) uncertainty\n",
        "For a maximum likelihood estimation we need to devise a likelihood function. Let's assume a Gaussian likelihood function, parametrized by a deep network to give its mean and variance, given an input. In this first part, we assume the deep network produces a fixed variance independent of the input sample while the mean is dependent on the input sample. That is, our likelihood function has the following form:\n",
        "\n",
        "\\begin{align}\n",
        "P(y|\\mathbf{x};\\mathbf{\\theta}) = \\mathcal{N}(\\mu=f_{\\mathbf{\\theta}}(\\mathbf{x}),\\sigma^2=f'_\\theta).\n",
        "\\end{align}\n",
        "\n",
        "Think(!) how this is the same as assuming an additive Gaussian noise with a fixed variance on the labels.\n",
        "\n",
        "First, we consider an additional parameter for the fixed variance and initialize the rest of the network parameters as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3lKIVzjNb1I"
      },
      "outputs": [],
      "source": [
        "# assume one scalar for the fixed standard dev of the likelihood's Gaussian\n",
        "sigma_param = jnp.array(jnp.log(0.2))\n",
        "\n",
        "# Initialize the network parameters that will produce the mean of the likelihood's Gaussian\n",
        "params_homo_model_clean_data = standard_fc_network_init(input_dim,\n",
        "                                                        layers_width, \n",
        "                                                        output_dim, \n",
        "                                                        key)\n",
        "\n",
        "# add the likelihood's Gaussian's standard dev to the list of parameters\n",
        "params_homo_model_clean_data.append(sigma_param)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params_homo_model_clean_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W7632TyDWlSX"
      },
      "source": [
        "Then, we would like to find $\\theta$ that maximizes the likelihood of our observed data:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{\\theta}^*=\\text{argmax}_\\mathbf{\\theta}\\prod_{i=1}^{n}P(y_i|\\mathbf{x}_i;\\mathbf{\\theta}).\n",
        "\\end{align}\n",
        "\n",
        "Taking the log and plugging in the Normal distributions we get:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{\\theta}^*=\\text{argmax}_\\mathbf{\\theta}\\sum_{i=1}^{n}\\log \\mathcal{N}(\\mu=f_{\\mathbf{\\theta}}(\\mathbf{x}_i),\\sigma^2=f'_\\theta)|_{y_i}= \\text{argmin}_\\mathbf{\\theta}[...]\n",
        "\\end{align}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GDLinPdmTiEk"
      },
      "source": [
        "**========================**\n",
        "####**TODO**: Complete Equations.\n",
        "\n",
        "**========================**\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly6kKO10TjLy"
      },
      "source": [
        "Your job is now to fill in the missing part above and implement the corresponding loss function. Note that the last term is an *argmin*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_OfN_eILbxV"
      },
      "outputs": [],
      "source": [
        "def loss_mle_homoscedastic(params, X, Y):\n",
        "    \"\"\"\n",
        "    ==============================\n",
        "    TODO: Implementation required.\n",
        "    ==============================\n",
        "    1. Assume params[-1] contains the natural logarithm of the standard_deviation\n",
        "    2. Return the total loss calculated as you derived above.\n",
        "    \"\"\"\n",
        "    preds = standard_fc_network_forward(X, params[:-1])\n",
        "    \n",
        "    raise NotImplementedError(\"Task: Implement!\")  \n",
        "    loss = NotImplemented  \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjl_6rIrTK--"
      },
      "outputs": [],
      "source": [
        "params_homo_model_clean_data,\\\n",
        "train_loss_homo_model_clean_data,\\\n",
        "train_mse_homo_model_clean_data,\\\n",
        "test_mse_homo_model_clean_data = run_training_loop(num_epochs, \n",
        "                                                   opt_state, X_train, \n",
        "                                                   Y_train_true, X_test, \n",
        "                                                   Y_test_true, \n",
        "                                                   len(layers_width)+1, \n",
        "                                                   loss_mle_homoscedastic)\n",
        "\n",
        "sigma_param = jnp.exp(params_homo_model_clean_data[-1])\n",
        "print('The final trained homoscedastic standard deviation'\n",
        "      ' for clean data is: ', \n",
        "      sigma_param)\n",
        "\n",
        "Y_test_pred_homo_model_clean_data = standard_fc_network_forward(X_test.T,\n",
        "                                                                params_homo_model_clean_data[:-1])\n",
        "\n",
        "visulize_predictions(Y_test_pred_homo_model_clean_data,  \n",
        "                     r'Predictions of a $\\bf{Homoscedastic\\, FC\\, network}$'\n",
        "                     r' trained on training data with $\\bf{clean\\, labels}$', \n",
        "                     X_train, \n",
        "                     Y_train_true, \n",
        "                     sigma_param)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "69e29sA1xJuY"
      },
      "source": [
        "The model with homoscedastic noise should learn a function that has negligible uncertainty. \n",
        "\n",
        "Now, let us try it on the noisy data with fixed (homoscedastic) additive noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4b7fsaaxN-W"
      },
      "outputs": [],
      "source": [
        "# assume one scalar for the fixed standard dev of the likelihood's Gaussian\n",
        "sigma_param = jnp.array(jnp.log(0.2))\n",
        "\n",
        "# Initialize the network parameters that will produce the mean of the likelihood's Gaussian\n",
        "params_homo_model_homo_data = standard_fc_network_init(input_dim, \n",
        "                                                       layers_width, \n",
        "                                                       output_dim, \n",
        "                                                       key)\n",
        "\n",
        "# add the likelihood's Gaussian's standard dev to the list of parameters\n",
        "params_homo_model_homo_data.append(sigma_param)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params_homo_model_homo_data)\n",
        "\n",
        "params_homo_model_homo_data,\\\n",
        "train_loss_homo_model_homo_data,\\\n",
        "train_mse_homo_model_homo_data,\\\n",
        "test_mse_homo_model_homo_data = run_training_loop(num_epochs, \n",
        "                                                  opt_state, \n",
        "                                                  X_train,\n",
        "                                                  Y_train_hom, \n",
        "                                                  X_test,\n",
        "                                                  Y_test_true, \n",
        "                                                  len(layers_width)+1,\n",
        "                                                  loss_mle_homoscedastic)\n",
        "\n",
        "sigma_param = jnp.exp(params_homo_model_homo_data[-1])\n",
        "print('The final trained homoscedastic standard deviation '\n",
        "      'for data with fixed additive Gaussian noise is: ',\n",
        "      sigma_param)\n",
        "\n",
        "Y_test_pred_homo_model_homo_data = standard_fc_network_forward(X_test.T,\n",
        "                                                               params_homo_model_homo_data[:-1])\n",
        "\n",
        "visulize_predictions(Y_test_pred_homo_model_homo_data,\n",
        "                     r'Predictions of a $\\bf{Homoscedastic\\, FC\\, network}$'\n",
        "                     r' trained on training data with $\\bf{homoscedastic\\, noise}$',\n",
        "                     X_train,\n",
        "                     Y_train_hom,\n",
        "                     sigma_param)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwDOc9Z9QuC"
      },
      "source": [
        "Are you happy with the predicted uncertainty? Does the uncertainty shading contain most observed datapoints? \n",
        "\n",
        "Now, let us try it on the data with variable (heteroscedastic) label noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYa2kn9p9WMG"
      },
      "outputs": [],
      "source": [
        "# assume one scalar for the fixed standard dev of the likelihood's Gaussian\n",
        "sigma_param = jnp.array(jnp.log(0.2))\n",
        "\n",
        "# Initialize the network parameters that will produce the mean of the likelihood's Gaussian\n",
        "params_homo_model_het_data = standard_fc_network_init(input_dim, \n",
        "                                                      layers_width,\n",
        "                                                      output_dim, \n",
        "                                                      key)\n",
        "\n",
        "# add the likelihood's Gaussian's standard dev to the list of parameters\n",
        "params_homo_model_het_data.append(sigma_param)\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params_homo_model_het_data)\n",
        "\n",
        "params_homo_model_het_data,\\\n",
        "train_loss_homo_model_het_data,\\\n",
        "train_mse_homo_model_het_data,\\\n",
        "test_mse_homo_model_het_data = run_training_loop(num_epochs, \n",
        "                                                 opt_state,\n",
        "                                                 X_train,\n",
        "                                                 Y_train_het,\n",
        "                                                 X_test,\n",
        "                                                 Y_test_true,\n",
        "                                                 len(layers_width)+1, \n",
        "                                                 loss_mle_homoscedastic)\n",
        "\n",
        "sigma_param = jnp.exp(params_homo_model_het_data[-1])\n",
        "print('The final trained homoscedastic standard deviation '\n",
        "      'for data with variable (heteroscedastic) noise is: ',\n",
        "      sigma_param)\n",
        "\n",
        "Y_test_pred_homo_model_het_data = standard_fc_network_forward(X_test.T, params_homo_model_het_data[:-1])\n",
        "\n",
        "visulize_predictions(Y_test_pred_homo_model_het_data,\n",
        "                     r'Predictions of a $\\bf{Homoscedastic\\, FC\\, network}$'\n",
        "                     r' trained on training data with $\\bf{heteroscedastic\\, noise}$',\n",
        "                     X_train, \n",
        "                     Y_train_het, \n",
        "                     sigma_param)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MpcbefYFU41b"
      },
      "source": [
        "How about now? Are you as happy with the predicted uncertainty? Does the uncertainty shading contain similar amount of observed datapoints? If not as happy, we will re-explore this idea with heteroscedastic noise modeling in another implementation practical, so stay tuned!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GhPs8QyKPAEY"
      },
      "source": [
        "# Variational Inference for Deep Regression Networks\n",
        "In this part of the practical we are interested in epistemic uncertainty estimation with deep networks based on the following paper.\n",
        "\n",
        "[1] Blundell et al., \"Weight Uncertainty in Neural Networks\", ICML 2015\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XXhaD8QljWZd"
      },
      "source": [
        "\n",
        "\n",
        "In order to model epistemic uncertainty, we can use Bayesian modelling to obtain a posterior distribution on model parameters:\n",
        "\n",
        "\\begin{align}\n",
        "P(\\mathbf{\\theta}|\\mathcal{D})= \\frac{P(\\mathcal{D}|\\mathbf{\\theta})P(\\mathbf{\\theta})}{P(\\mathcal{D})}\n",
        "\\end{align}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JjfwKT7Tj1UL"
      },
      "source": [
        "\n",
        "Using this posterior distribution, we can transfer the uncertainty in model parameters to uncertainty in predictive distribution, where, for a new test sample $\\mathbf{x}$:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qrChgRgfjzoL"
      },
      "source": [
        "\\begin{align}\n",
        "  P(y|\\mathbf{x},\\mathcal{D})=\\int P(y|\\mathbf{x},\\mathbf{\\theta})P(\\mathbf{\\theta}|\\mathcal{D})d\\theta.\n",
        "\\end{align}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PDkiPW1pjyUt"
      },
      "source": [
        "However, obtaining $P(\\mathbf{\\theta}|\\mathcal{D})$ can be problematic due to the interactability of calculating $P(\\mathcal{D})$. Thus there are several techniques developed to approximate the posterior distribution $P(\\mathbf{\\theta}|\\mathcal{D})$. One of these techniques is called Variational Inference (VI). In VI, we seek to identify another, simpler, distribution $Q_\\omega(\\theta)$, parametrized by $\\omega$, that approximates $P(\\theta|\\mathcal{D})$. In VI we can maximize the variational lower bound (ELBO)(or minimize a negated version) as follows (details provided in the course lectures and the VAE practical):\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JyU6GbAGjwtD"
      },
      "source": [
        "\\begin{align}\n",
        "\\omega^*=\\text{argmin}_{\\omega} -\\mathbb{E}_{Q_\\omega(\\theta)}[\\log P(\\mathcal{D}|\\theta)]+\\text{D}_{\\text{KL}}(Q_\\omega(\\theta)||P(\\theta))\n",
        "\\end{align}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ho2Tf1DXju2y"
      },
      "source": [
        "Assuming i.i.d. dataset $\\mathcal{D}$ we will have:\n",
        "\n",
        "\\begin{align}\n",
        "\\omega^*=\\text{argmin}_{\\omega} -\\mathbb{E}_{Q_\\omega(\\theta)}[\\sum^{n}_{i=1}\\log P(y|\\mathbf{x},\\theta)]+\\text{D}_{\\text{KL}}(Q_\\omega(\\theta)||P(\\theta))\n",
        "\\end{align}\n",
        "\n",
        "Using monte-carlo estimation of the expecation with $S$ samples we get:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T0B6ngzPjtD0"
      },
      "source": [
        "\\begin{align}\n",
        "\\omega^*=\\text{argmin}_{\\omega} -\\frac{1}{S}\\sum_{s=1}^{S}\\sum^{n}_{i=1}\\log P(y|\\mathbf{x},\\theta_s)+\\text{D}_{\\text{KL}}(Q_\\omega(\\theta)||P(\\theta)), \\tag{2}\n",
        "\\end{align}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8_U_VoQUjq8C"
      },
      "source": [
        "where, $\\{\\theta_s\\}_{s=1}^{S}\\sim Q_\\omega(\\theta)$.\n",
        "\n",
        "Let us consider the following distributions:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TvfWg3aGjoU_"
      },
      "source": [
        "*   $P(\\theta)=\\mathcal{N}(\\mathbf{0}_p, a^2\\mathbf{I}_p)\\qquad\\qquad\\qquad\\;\\;$ with $\\;a^2=10^{-4}$\n",
        "*   $Q_{\\omega=(\\mu,\\sigma^2)}(\\theta)=\\mathcal{N}(\\mathbf{\\mu}, diag(\\mathbf{\\sigma}^2))\\qquad$ with $\\;\\mu \\in \\mathbb{R}^{p}\\;$ and $\\;\\sigma^2 \\in \\mathbb{R}_{+}^{p}$\n",
        "*   $P(y|\\mathbf{x},\\theta)=\\mathcal{N}(f_{\\theta}^{\\mu}(\\mathbf{x}), f_{\\theta}^{\\sigma^2}(\\mathbf{x})) \\qquad\\;$ with $\\;f_{\\theta}^{\\mu}(\\mathbf{x}) \\in \\mathbb{R}\\;$ and $\\;f_{\\theta}^{\\sigma^2}(\\mathbf{x}) \\in \\mathbb{R}_{+}$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wmk4We9pjmbS"
      },
      "source": [
        "where $f_\\theta$ denotes the VI deep network's function. $f_{\\theta}^{\\mu}(\\mathbf{x})$ and $f_{\\theta}^{\\sigma^2}(\\mathbf{x})$ denote the mean and variance outputs repsectively from the deep network for an input $\\mathbf{x}$. Note that $P(\\theta)$ and $Q_{\\omega}(\\theta)$ have a diagonal covariance matrix.\n",
        "\n",
        "Following [1], to avoid numerical issues, in the implementation we will have the deep network to output $f_{\\theta}^{\\rho}(\\mathbf{x}))$ instead and we obtain the variance using the softplus operation as:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HuqTQihgjkkK"
      },
      "source": [
        "\\begin{align}\n",
        "  f_{\\theta}^{\\sigma}=\\text{log}(1+\\exp(f_{\\theta}^{\\rho})) \n",
        "\\end{align}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sswgn0tPjgbK"
      },
      "source": [
        "Now, your job is to plug in the distributions above into the objective in equation 2 assuming a single-sample monte-carlo estimate ($S=1$). Note that the KL divergence of two Gaussians have a closed-from solution and the natural logarithm of a Gaussian gives away two simple terms and a constant. Fill in the following objective equation with your derived terms:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B0jYftBzjd_T"
      },
      "source": [
        "\n",
        "\\begin{align}\n",
        "\\omega^*=\\text{argmin}_{\\omega} ...\n",
        "\\end{align}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j9uJAcrdjip7"
      },
      "source": [
        "**========================**\n",
        "####**TODO**: Complete Equations.\n",
        "\n",
        "**========================**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oaBsF2PLjb-6"
      },
      "source": [
        "Furthermore, we saw in the class that backpropagating through a stochastic node is problematic. One simple remedy, among others [2], to this problem is called reparametrization trick. There, for a Gaussian distribution, one can sample from a fixed (non-learnable) standard distribution and then transfer the sample into a sample of the original distribution by\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zr-4P2f6jZ-S"
      },
      "source": [
        "\\begin{align}\n",
        "  \\theta_s= \\sigma\\otimes\\epsilon + \\mu \\qquad \\text{with }\\; \\epsilon \\sim \\mathcal{N}(\\mathbf{0}_p, \\mathbf{I}_p)\n",
        "\\end{align}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bUR2AJj3jYaG"
      },
      "source": [
        "where $\\otimes$ denotes an elementwise (Hadamard) product.\n",
        "\n",
        "In the following, we consider a fixed aleatoric uncertainty where we set $f^{\\sigma^2}_\\hat{\\theta}(\\mathbf{x}_i)=c^2$ that means our likelihood function takes the form $P(y|\\mathbf{x},\\theta)=\\mathcal{N}(f_{\\theta}^{\\mu}(\\mathbf{x}), c^2)$. Next, you will need to implement the objective function that you derived above using $c^2=1$ and the reparametrization trick.\n",
        "\n",
        "---\n",
        "[1] Blundell et al., \"Weight Uncertainty in Neural Networks\", ICML 2015\n",
        "\n",
        "[2] Bengio, et al., \"Estimating or Propagating Gradients\", 2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY9ySKV51FIe"
      },
      "outputs": [],
      "source": [
        "# VI network's configurations\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = Y_train_true.shape[1]\n",
        "layers_width = [100, 40, 100]\n",
        "kl_prior_mean = 0\n",
        "kl_prior_sigma = 1e-2\n",
        "kl_weight = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFIo_gO9GpMu"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters of a VI-based fully-connected layer\n",
        "def vi_fc_layer_init(input_dim, output_dim, key, scale=1e-2):\n",
        "    keys = random.split(key, 2)\n",
        "    weights_mean = random.normal(keys[0], (output_dim, input_dim)) * scale\n",
        "    weights_rho = -2*jnp.ones((output_dim, input_dim))\n",
        "    biases_mean  = random.normal(keys[1], (output_dim, 1)) * scale\n",
        "    biases_rho = -2*jnp.ones((output_dim, 1))\n",
        "    return weights_mean, weights_rho, biases_mean, biases_rho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HQJdeyr2OgC"
      },
      "outputs": [],
      "source": [
        "# Forward pass of a VI-based fully-connected layer\n",
        "def vi_fc_layer_forward(input, weights_mean, weights_rho, biases_mean, biases_rho, key):\n",
        "    \"\"\"\n",
        "        ==============================\n",
        "        TODO: Implementation required.\n",
        "        ==============================\n",
        "        Implement the reparametrization trick:\n",
        "        1. For weights and biases:\n",
        "        1.1 Sample from a standard Gaussian distribution using random.normal\n",
        "        1.2 Transform the sample into a sample of the desired Gaussian distribution\n",
        "        2. Return the affine transformation of the input using the weights and biases\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Task: Implement!\")\n",
        "    weights = NotImplemented\n",
        "    biases = NotImplemented\n",
        "    output = NotImplemented\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt9ug7yTy_n-"
      },
      "outputs": [],
      "source": [
        "# Initialize a VI-based fully-connected network \n",
        "def vi_fc_network_init(input_dim, layers_width, output_dim, key):\n",
        "  network_feature_dims = [input_dim, *layers_width, output_dim]\n",
        "  keys = random.split(key, len(network_feature_dims[:-1]))\n",
        "  params = [vi_fc_layer_init(network_feature_dims[i],\n",
        "                             network_feature_dims[i+1], \n",
        "                             keys[i]) \n",
        "            for i in range(len(network_feature_dims[:-1]))]\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9SThKEdJgyD"
      },
      "outputs": [],
      "source": [
        "# Forward pass of a VI-based fully-connected network \n",
        "def vi_fc_network_forward(input, params, key, is_classification=False):\n",
        "\n",
        "    representation = input\n",
        "    keys = random.split(key, len(params))\n",
        "\n",
        "    for (weights_mean, weights_rho, biases_mean, biases_rho), key in zip(params[:-1], keys[:-1]):\n",
        "        representation = nn.relu(vi_fc_layer_forward(representation, weights_mean, weights_rho, biases_mean, biases_rho, key))\n",
        "    representation = vi_fc_layer_forward(representation, params[-1][0], params[-1][1], params[-1][2], params[-1][3], keys[-1]) \n",
        "    \n",
        "    return representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeAcJgtCHxuy"
      },
      "outputs": [],
      "source": [
        "params = vi_fc_network_init(input_dim, \n",
        "                            layers_width,\n",
        "                            output_dim, \n",
        "                            key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF5jacY4BNsn"
      },
      "outputs": [],
      "source": [
        "def kl_divergence_two_diagonal_gaussians(q_mu, q_sigma, p_mu, p_sigma):        \n",
        "    \"\"\"\n",
        "    calculates the KL-divergence of two diagnoal gaussians.\n",
        "    For convenience, p_mu and p_sigma can be scalars.\n",
        "    \"\"\"\n",
        "    q_mu = q_mu.flatten()\n",
        "    q_sigma = q_sigma.flatten()\n",
        "\n",
        "    N = q_mu.shape[0]\n",
        "\n",
        "    p_sigma *= p_sigma\n",
        "    q_sigma *= q_sigma\n",
        "\n",
        "    if jnp.isscalar(p_mu):\n",
        "        p_mu = p_mu * jnp.ones_like(q_mu) \n",
        "        p_sigma = p_sigma * jnp.ones_like(q_mu)\n",
        "    else:\n",
        "        p_mu = p_mu.flatten()\n",
        "        p_sigma = p_sigma.flatten()\n",
        "\n",
        "\n",
        "    p_mu = p_mu.reshape(N)\n",
        "    q_mu = q_mu.reshape(N)\n",
        "    p_sigma = p_sigma.reshape(N)\n",
        "    q_sigma = q_sigma.reshape(N)\n",
        "\n",
        "    ip_sigma = 1/p_sigma\n",
        "    diff = p_mu - q_mu\n",
        "\n",
        "    tr_term   = jnp.sum(ip_sigma * q_sigma)\n",
        "    det_term  = jnp.sum(jnp.log(p_sigma/q_sigma))\n",
        "    quad_term = diff.T @ jnp.diag(1/p_sigma) @ diff \n",
        "\n",
        "    return .5 * (tr_term + det_term + quad_term - N) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXZqUyaGsrc7"
      },
      "outputs": [],
      "source": [
        "def vi_loss(params, X, Y, prior_mu, prior_sigma, kl_weight, key, is_classification=False):\n",
        "    \"\"\"\n",
        "    ==============================\n",
        "    TODO: Implementation required.\n",
        "    ==============================\n",
        "    Implement the VI loss function you derived in the instructions\n",
        "    1. Use the provided implementation of the KL divergence of two Gaussians in kl_divergence_two_diagnoal_gaussians (or implement one if you want)\n",
        "    2. Complete the loss function calculation below\n",
        "    3. For better learning use kl_weight as a coefficient to downweigh the KL term (prior), i.e., - expected_likelihood_term + kl_wieght*kl_prior_term  \n",
        "    \"\"\"\n",
        "    preds = vi_fc_network_forward(X, params, key)\n",
        "\n",
        "    raise NotImplementedError(\"Task: Implement!\")\n",
        "    total_loss = NotImplemented\n",
        "    return total_loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aEaVYGwN6xR"
      },
      "outputs": [],
      "source": [
        "# Compute the gradient for a batch and update the parameters\n",
        "@jit\n",
        "def update(params, X, Y, opt_state, key):\n",
        "    value, grads = value_and_grad(vi_loss)(params, \n",
        "                                           X, \n",
        "                                           Y,\n",
        "                                           kl_prior_mean, \n",
        "                                           kl_prior_sigma, \n",
        "                                           kl_weight, \n",
        "                                           key)\n",
        "    opt_state = opt_update(0, grads, opt_state)\n",
        "    return get_params(opt_state), opt_state, value\n",
        "\n",
        "# Defining an optimizer in Jax\n",
        "step_size = 1e-2\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "num_epochs = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxnzL3vfPWrI"
      },
      "outputs": [],
      "source": [
        "# Implements a learning loop over epochs. \n",
        "def run_training_loop(num_epochs, opt_state, X, Y, X_test, Y_test):\n",
        "    # Initialize placeholder for logging\n",
        "    train_loss, test_loss = [], []\n",
        "\n",
        "    # Get the initial set of parameters\n",
        "    params = get_params(opt_state)\n",
        "\n",
        "    keys = random.split(key)\n",
        "\n",
        "    # Get initial accuracy after random init\n",
        "    train_loss.append(vi_loss(params,\n",
        "                              X.T, \n",
        "                              Y.T, \n",
        "                              kl_prior_mean,\n",
        "                              kl_prior_sigma,\n",
        "                              kl_weight, \n",
        "                              keys[0]))\n",
        "    test_loss.append(vi_loss(params, \n",
        "                             X_test.T,\n",
        "                             Y_test.T,\n",
        "                             kl_prior_mean, \n",
        "                             kl_prior_sigma,\n",
        "                             kl_weight,\n",
        "                             keys[1]))\n",
        "\n",
        "    keys = random.split(key, num_epochs*2)\n",
        "\n",
        "    # Loop over the training epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        params, opt_state, loss_val = update(params, \n",
        "                                             X.T, \n",
        "                                             Y.T,\n",
        "                                             opt_state,\n",
        "                                             keys[epoch*2])\n",
        "        train_loss.append(loss_val)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        test_loss.append(vi_loss(params,\n",
        "                                 X_test.T,\n",
        "                                 Y_test.T,\n",
        "                                 kl_prior_mean,\n",
        "                                 kl_prior_sigma,\n",
        "                                 kl_weight, \n",
        "                                 keys[epoch*2+1]))\n",
        "        if epoch%100 == 0:\n",
        "            print(\"Epoch {} | T: {:0.6f} | Train loss: {:0.3f}\"\n",
        "                    \" | Test loss: {:0.3f}\".format(epoch+1, \n",
        "                                               epoch_time,\n",
        "                                               train_loss[-1], \n",
        "                                               test_loss[-1]))\n",
        "\n",
        "    return params, train_loss, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJxQavFhpJDN"
      },
      "outputs": [],
      "source": [
        "def visualize_VI(params, X_train, Y_train, plot_title, num_trials = 10):\n",
        "  \n",
        "    Y_test_pred_list = []\n",
        "\n",
        "    keys = random.split(key, num_trials)\n",
        "\n",
        "    for i in tqdm.tqdm(range(num_trials)):\n",
        "        Y_test_pred = vi_fc_network_forward(X_test.T, params, keys[i])\n",
        "        Y_test_pred_list.append(Y_test_pred)\n",
        "        \n",
        "    Y_test_preds = np.concatenate(Y_test_pred_list, axis=0)\n",
        "    print(Y_test_preds.shape)\n",
        "\n",
        "    Y_test_pred_mean = np.mean(Y_test_preds, axis=0)\n",
        "    Y_test_pred_sigma = np.std(Y_test_preds, axis=0)\n",
        "    visulize_predictions(Y_test_pred_mean,\n",
        "                        plot_title,\n",
        "                        X_train,\n",
        "                        Y_train,\n",
        "                        Y_test_pred_sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OxXpkgRnqxy"
      },
      "outputs": [],
      "source": [
        "params,\\\n",
        "train_loss,\\\n",
        "test_loss = run_training_loop(num_epochs,\n",
        "                              opt_state, \n",
        "                              X_train,\n",
        "                              Y_train_true, \n",
        "                              X_test,\n",
        "                              Y_test_true)\n",
        "visualize_VI(params,\n",
        "             X_train, \n",
        "             Y_train_true, \n",
        "             r'Predictions of a $\\bf{VI\\, FC\\, network}$'\n",
        "             r' trained on $\\bf{clean\\, training\\, data}$', \n",
        "             num_trials = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCbBe28GppWe"
      },
      "outputs": [],
      "source": [
        "params,\\\n",
        "train_loss,\\\n",
        "test_loss = run_training_loop(num_epochs,\n",
        "                              opt_state, \n",
        "                              X_train, \n",
        "                              Y_train_hom,\n",
        "                              X_test,\n",
        "                              Y_test_true)\n",
        "visualize_VI(params,\n",
        "             X_train,\n",
        "             Y_train_hom,\n",
        "             r'Predictions of a $\\bf{VI\\, FC\\, network}$'\n",
        "             r' trained on training data with $\\bf{homoscedastic\\, noise}$',\n",
        "             num_trials = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEoVlyHvKyrc"
      },
      "outputs": [],
      "source": [
        "params,\\\n",
        "train_loss,\\\n",
        "test_loss = run_training_loop(num_epochs,\n",
        "                              opt_state, \n",
        "                              X_train, \n",
        "                              Y_train_het,\n",
        "                              X_test,\n",
        "                              Y_test_true)\n",
        "visualize_VI(params,\n",
        "             X_train,\n",
        "             Y_train_het,\n",
        "             r'Predictions of a $\\bf{VI\\, FC\\, network}$'\n",
        "             r' trained on training data with $\\bf{heteroscedastic\\, noise}$',\n",
        "             num_trials = 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E44xstX9q4oy"
      },
      "source": [
        "**Note 1.** A proper implementation would show growing uncertainty as we move away from the training region irrespective of existence and type of noise. This is essentially what we expect of epistemic uncertainty, i.e., showing increased uncertainty as the test data get further away from the observed training data!\n",
        "\n",
        "**Note 2.** The aleatoric uncertainty is still not perfectly captured and there can be too much uncertainty for observed (clean) data. However, remember we used the a very simplistic way of modeling Bayesian uncertainty which can be improved using the advanced and more recent techniques. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q23yx045Q8Mb"
      },
      "source": [
        "Finally, it is valuable for us to know how long did it take you to complete this practical?\n",
        "\n",
        "The full practical took us [**to be filed in**] hours to be completed."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
